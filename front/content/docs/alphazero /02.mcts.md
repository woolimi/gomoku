---
group: "AlphaZero"
icon: "pi-sitemap"
---

# Monte Carlo Tree Search with PUCT

## MCTS vs Minimax

Unlike minimax, which exhaustively searches to a fixed depth with a uniform branching strategy, MCTS builds a search tree **asymmetrically** — spending more time on promising moves and less on obviously bad ones. There's no fixed search depth; instead, the algorithm runs a fixed number of simulations, each traversing from root to a leaf, evaluating, and backing up.

After $N$ simulations, the action policy at the root is proportional to visit counts: $\pi(a) \propto N(a)$. More visits to a child node means more confidence that it's a good move.

## Four Phases

Each MCTS simulation has four phases:

1. **Select**: Starting from the root, traverse down the tree by choosing the child with the highest UCB score at each node. This balances exploitation (high value) with exploration (low visit count, high prior).

2. **Evaluate**: Run neural network inference on the leaf state to get policy logits and a value estimate. The logits are converted to a legal-only prior distribution with softmax (and root Dirichlet noise during self-play).

3. **Expand**: After inference, expand the leaf by creating child nodes for all legal moves. Child priors come from the evaluated policy distribution.

4. **Backup**: Propagate the value estimate from the leaf back to the root. At each level, the sign flips — the opponent's gain is our loss:

```python
# alphazero/gomoku/pvmcts/treenode.py:217-235
def backup(self, value: float) -> None:
    node = self
    while node is not None:
        node.value_sum += value
        node.visit_count += 1
        value = -value  # flip for opponent
        node = node.parent
```

In this codebase, Evaluate and Expand are implemented together in `_evaluate_and_expand(...)`: inference runs first, then expansion uses the returned policy priors.

<!-- DIAGRAM_03: MCTS 4-phase simulation loop. Show a partial game tree with root (visit_count=N) and several children at depth 1, one of which has depth-2 children. Label each phase as an annotated step on the diagram: (1) SELECT — bold arrow tracing the highest-UCB path from root to a leaf node (show UCB = Q + c·P·√N/(1+n) at an edge; flip sign at each level because child is opponent); (2) EVALUATE — arrow from leaf to a NN box, returning policy logits and value scalar v; Dirichlet noise is shown only at root (applied before selection during self-play); (3) EXPAND — the leaf node creates child nodes for all legal moves using prior probabilities p1…pK from the evaluated policy; (4) BACKUP — value v propagates back along the selected path, with sign flip at each level (leaf gets +v, parent gets -v, grandparent gets +v), incrementing visit_count and value_sum at each node. -->

<div class="diagram-embed-wrap">
  <iframe
    class="diagram-embed"
    src="/images/diagrams/diagram_03/index.html"
    title="MCTS Four-Phase Simulation"
    loading="lazy"
  ></iframe>
</div>

<p class="diagram-embed-link">
  <a href="/images/diagrams/diagram_03/index.html" target="_blank" rel="noopener noreferrer">
    Open in a new tab
  </a>
</p>

## PUCT Selection Formula

The selection step uses the PUCT (Predictor + UCB applied to Trees) formula:

$$
UCB(s,a) = Q(s,a) + c_{\text{puct}} \cdot P(s,a) \cdot \frac{\sqrt{N(s)}}{1 + N(s,a)}
$$

where:
- $Q(s,a)$ = mean value of the child (average backed-up value)
- $P(s,a)$ = prior probability from the neural network's policy head
- $N(s)$ = visit count of the parent
- $N(s,a)$ = visit count of the child
- $c_{\text{puct}}$ = exploration constant (2.0 in this implementation)

Why not plain UCT? Classical UCT uses an exploration bonus based only on visit counts (typically $\sqrt{\log N(s) / (1 + N(s,a))}$), so all moves start nearly equal before enough rollouts accumulate. On a 19x19 board (361 actions), that is too sample-inefficient. PUCT injects the network prior $P(s,a)$ into the exploration term, so search spends simulations on moves the policy already considers plausible while still exploring under-visited branches.

This formula balances **exploitation** (high $Q$ — moves that have led to good outcomes) with **exploration** (high prior $P$, low visits $N(s,a)$ — moves the network thinks are promising but haven't been explored much yet). The neural network's policy prior guides the search toward promising moves before they've been visited enough to have reliable $Q$ values — this is what makes MCTS + neural network so much more powerful than MCTS with random rollouts.

```python
# alphazero/gomoku/pvmcts/treenode.py:102-128
def get_ucb(self, c_puct: float) -> float:
    if self.parent is None:
        return 0.0
    parent_visits = self.parent.visit_count + self.parent.pending_visits
    q_value = 0.0
    total_child_visits = self.visit_count + self.pending_visits
    if total_child_visits > 0:
        q_value = self.value_sum / total_child_visits
    q_value = -q_value  # opponent's perspective
    exploration = (c_puct * self.prior
                   * math.sqrt(parent_visits)
                   / (1 + total_child_visits))
    return q_value + exploration
```

Note the sign flip: `q_value = -q_value` because the child represents the opponent's perspective. A high value for the child is bad for the parent.

## Dirichlet Noise

At the root node only, Dirichlet noise is added to the priors before search begins:

$$
P'(s,a) = (1 - \epsilon) \cdot P(s,a) + \epsilon \cdot \eta_a, \quad \eta \sim \text{Dir}(\alpha)
$$

This ensures exploration during self-play training: even if the network strongly favors one move, the noise gives other moves a chance to be explored. Without noise, the search would converge to the network's current beliefs and never discover better moves.

In this codebase, Dirichlet is applied only at the root during self-play (`add_noise=True`), after legal-move masking + softmax, and only over legal actions (then renormalized). See `alphazero/gomoku/pvmcts/search/engine.py` (`_masked_policy_from_logits`, `_safe_dirichlet_noise`, `_ensure_root_noise`).

Parameters: $\epsilon = 0.25$ follows the AlphaZero paper; $\alpha$ was tuned for this 19x19 board (the theoretical optimum is $\sim 10 / \text{avg\_legal\_moves}$).

In this project's training history, $\alpha$ varied by phase (baseline `0.15`, experimental low `0.03` during Days 6.5–7). In evaluation/production, Dirichlet noise is disabled (`dirichlet_epsilon = 0.0`), so $\alpha$ is effectively unused.

## Temperature-Based Action Selection

After MCTS completes, the root visit counts form a policy distribution. Temperature is applied to this **visit-based policy** (not to network logits). How that distribution is used depends on the training phase:

$$
\pi(a) \propto N(a)^{1/\tau}
$$

During the first 20 moves (configurable `exploration_turns`), temperature $\tau = 1.0$ — actions are sampled proportionally to visit counts, encouraging diverse opening play for the training data. After the exploration phase, $\tau \to 0$ (effectively argmax — pick the most-visited move), which produces the strongest deterministic play.

Implementation detail: temperature is applied in `AlphaZeroAgent._apply_temperature` (`alphazero/gomoku/alphazero/agent.py`) with stable guards (near-zero → argmax one-hot, otherwise exponentiate by $1/\tau$ and renormalize).

During evaluation and production serving, this project uses deterministic settings (`temperature=0`, no Dirichlet noise) for strongest play.

Practical split of responsibilities:
- **Dirichlet noise** perturbs root priors before/during search (exploration at search time).
- **Temperature** reshapes the final root visit distribution after search (exploration at action-selection time).
