---
group: "AlphaZero"
icon: "pi-cog"
---

# Search Engine Architecture: From Sequential to Distributed

## Strategy Pattern Overview

MCTS requires many neural network inferences per move — hundreds to thousands of forward passes through the ResNet for each game position. The optimal strategy for managing these inferences depends on the hardware: single CPU, local GPU, multiple processes, or a distributed Ray cluster. Rather than hardcoding one approach, the system uses a **strategy pattern**: all engines implement the `SearchEngine` abstract base class, and the `PVMCTS` facade selects the right one based on configuration.

Four modes are available:

- **Sequential**: One simulation at a time, synchronous neural network inference. Used for development, debugging, and CPU-based production serving. Supports native C++ MCTS via `CppSearchStrategy`.
- **Vectorize**: Interleaves multiple tree traversals and batches leaf evaluations into a single GPU call. Optimizes GPU utilization for local training by amortizing inference overhead across multiple simultaneous searches.
- **Multiprocess**: Distributes MCTS simulations across a process pool with a central inference server via IPC. Falls back to SequentialEngine when native C++ is enabled.
- **Ray**: Distributed across a Ray cluster. The most complex mode — uses an asynchronous pipelined architecture to overlap CPU tree traversal with remote GPU inference. Deep dive below.

Native C++ MCTS compatibility varies by mode:

| Mode | Native C++ MCTS |
|------|----------------|
| Sequential | Full support |
| Ray | Via SequentialEngine + CppSearchStrategy with async callbacks |
| Vectorize | Not supported |
| Multiprocess | Falls back to single-threaded SequentialEngine |

## The Core Problem: CPU/GPU Pipeline Stall

In a synchronous (stop-and-wait) approach, the CPU selects leaf nodes, bundles them into a batch, sends the batch to a remote GPU actor, and **blocks** until the result returns. Only then does the CPU resume tree traversal. This wastes both resources: the GPU sits idle during tree traversal, and the CPU sits idle during inference. With network latency to remote GPU actors, this idle time becomes the dominant bottleneck.

## The Solution: Pipelined Architecture

The key insight: overlap CPU work (tree selection and expansion) with GPU work (inference). While the GPU processes batch $N$, the CPU selects leaf nodes for batch $N+1$. This requires solving the **duplicate path problem**: without protection, multiple concurrent selections would target the same promising leaf before the first result returns, wasting simulations.

The solution has three components: `RayAsyncEngine` (orchestration), `BatchInferenceManager` (batching), and `RayInferenceClient` (dispatch).

## RayAsyncEngine

The engine orchestrates the main search loop with three phases per iteration:

**Selection phase**: Traverse the tree via UCB to find leaf nodes. Apply virtual loss (`pending_visits += 1` on every node in the path) to discourage future selections from following the same path. Encode the leaf state tensor and enqueue it to the batch manager. Guards prevent redundant work: skip if a leaf is already in-flight (`inflight_to_root` dict), and pause if queue pressure exceeds `max_pending` (backpressure).

**Dispatch phase**: Send ready batches to Ray actors (delegates to `BatchInferenceManager`).

**Drain phase**: Collect results via `ray.wait(num_returns=1)` — returns as soon as **any** batch completes, not waiting for all. For each result: expand the leaf (create children from policy), backup the value to root (sign-flipping propagation), and remove virtual loss (`pending_visits -= 1`). Freed in-flight slots trigger `dispatch_ready()` to send more batches immediately.

```python
# alphazero/gomoku/pvmcts/search/ray/ray_async.py:218-246
# Virtual loss application + enqueue with error rollback
for node in path:
    node.pending_visits += 1
try:
    encoded = self.game.get_encoded_state(leaf.state)
    tensor = torch.from_numpy(encoded).unsqueeze(0)
    self.manager.enqueue(tensor, leaf, root, path)
except Exception:
    # Rollback virtual loss if enqueue fails
    for node in path:
        node.pending_visits = max(0, node.pending_visits - 1)
    raise
```

This shows the careful engineering: apply virtual loss along the entire path, enqueue the leaf for inference, and if anything fails, rollback to prevent stale `pending_visits > 0` from corrupting future selections.

## BatchInferenceManager

The manager accumulates individual leaf tensors into batches and dispatches them to GPU actors:

```python
# alphazero/gomoku/pvmcts/search/ray/batch_inference_manager.py:154-161
def _should_dispatch(self) -> bool:
    if not self._queue:
        return False
    if len(self._queue) >= self._batch_size:
        return True
    elapsed = time.monotonic() - self._first_enqueue_time
    return elapsed >= self._max_wait_s
```

The dispatch condition uses OR logic: fire when the queue reaches `batch_size` **or** when `max_wait_ms` has elapsed since the first enqueued item. The timeout prevents latency spikes when only a few leaves are queued near the end of search.

The manager also enforces `max_inflight_batches` — when too many batches are in-flight, the selection phase pauses (backpressure: `max_pending = inflight_limit x batch_size`). This prevents unbounded memory growth from queued tensors.

## RayInferenceClient

The client dispatches inference requests to remote Ray GPU actors:

```python
# alphazero/gomoku/inference/ray_client.py:289-315
def infer_async(self, batch_tensor: torch.Tensor) -> ray.ObjectRef:
    # Least-loaded actor selection
    actor_idx = min(
        range(len(self._actors)),
        key=lambda i: self._pending_counts[i]
    )
    self._pending_counts[actor_idx] += 1
    ref = self._actors[actor_idx].infer.remote(batch_tensor)
    self._ref_to_actor[ref] = actor_idx
    return ref  # non-blocking
```

**Least-loaded actor selection** picks the actor with the fewest pending requests, distributing load evenly across GPU workers. The method returns a `ray.ObjectRef` immediately (non-blocking) — the caller proceeds with tree traversal while Ray handles the actual compute.

On the actor side, an AsyncIO consumer loop implements aggressive batching: it blocks on the first item, then non-blocking grabs until `batch_size` is reached or a deadline expires. A single `model(inputs)` forward pass runs over the concatenated batch, then results are split back to per-request futures.

## Virtual Loss Mechanism

Virtual loss is the mechanism that makes asynchronous MCTS correct, not just fast.

When a leaf is selected and enqueued for inference, every node on the path from root to leaf gets `pending_visits += 1`. The UCB calculation includes these pending visits:

$$
UCB = Q + c \cdot P \cdot \frac{\sqrt{N + N_{\text{pending}}}}{1 + N_{\text{child}} + N_{\text{child\_pending}}}
$$

This inflates the "visited" count for nodes along the path, making that path less attractive for the next selection. Combined with the `inflight_to_root` dict (which prevents selecting the exact same leaf twice), this naturally spreads concurrent selections across diverse paths in the tree.

When inference results return, `pending_visits -= 1` on all path nodes. Error handling is critical: rollback uses `max(0, pending_visits - 1)` to guard against negative counts. A `finally` block in `search()` cleans up all outstanding virtual loss on early exit (KeyboardInterrupt, exception), preventing stale `pending_visits > 0` from corrupting future searches on the same tree.

<!-- DIAGRAM_04: Pipelined async MCTS timeline — two horizontal swim lanes (CPU lane and GPU lane) scrolling left to right. CPU lane shows repeating loop of three labeled phases: [Selection: traverse tree via UCB → apply virtual loss +VL along path → encode leaf state → enqueue to BatchInferenceManager] then [Dispatch: BatchInferenceManager fires batch when queue≥batch_size OR max_wait_ms elapsed → sends to least-loaded Ray actor] then [Drain: ray.wait(num_returns=1) returns earliest result → expand leaf with policy priors → backup value to root with sign flips → remove virtual loss -VL]. GPU lane shows: [Batch 1 inference] overlapping in time with [CPU selecting leaves for Batch 2], then [Batch 2 inference] overlapping with [CPU selecting Batch 3 leaves]. Annotate: virtual loss makes in-flight paths less attractive to concurrent selections; inflight_to_root dict prevents selecting same leaf twice; CPU pauses (backpressure) when pending count exceeds max_pending = inflight_limit × batch_size. Show the key insight with a bracket: GPU never sits idle, CPU never blocks waiting. -->
