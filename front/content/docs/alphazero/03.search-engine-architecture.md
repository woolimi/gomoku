---
group: "AlphaZero"
icon: "pi-cog"
---

# Search Engine Architecture: From Sequential to Distributed

## Strategy Pattern Overview

MCTS requires many neural network inferences per move — hundreds to thousands of forward passes through the ResNet for each game position. The optimal strategy for managing these inferences depends on the hardware: single CPU, local GPU, multiple processes, or a distributed Ray cluster. Rather than hardcoding one approach, the system uses a **strategy pattern**: all engines implement the `SearchEngine` abstract base class, and the `PVMCTS` facade selects the right one based on configuration.

Four modes are available:

- **Sequential**: One simulation at a time, synchronous neural network inference. Used for development, debugging, and CPU-based production serving. Supports native C++ MCTS via `CppSearchStrategy`.
- **Vectorize**: Interleaves multiple tree traversals and batches leaf evaluations into a single GPU call. Optimizes GPU utilization for local training by amortizing inference overhead across multiple simultaneous searches.
- **Multiprocess**: Distributes MCTS simulations across a process pool with a central inference server via IPC. Falls back to `SequentialEngine` when native C++ is enabled; otherwise uses `MultiprocessEngine` normally.
- **Ray**: Distributed across a Ray cluster. The most complex mode — uses an asynchronous pipelined architecture to overlap CPU tree traversal with remote GPU inference. Deep dive below.

Native C++ MCTS compatibility varies by mode:

| Mode | Native C++ MCTS |
|------|----------------|
| Sequential | Full support |
| Ray | Via SequentialEngine + CppSearchStrategy with async callbacks |
| Vectorize | Not supported |
| Multiprocess | Falls back to `SequentialEngine` when native C++ is enabled; otherwise uses `MultiprocessEngine` |

## The Core Problem: CPU/GPU Pipeline Stall

In a synchronous (stop-and-wait) approach, the CPU selects leaf nodes, bundles them into a batch, sends the batch to a remote GPU actor, and **blocks** until the result returns. Only then does the CPU resume tree traversal. This wastes both resources: the GPU sits idle during tree traversal, and the CPU sits idle during inference. With network latency to remote GPU actors, this idle time becomes the dominant bottleneck.

## The Solution: Pipelined Architecture

The key insight: overlap CPU work (tree selection and expansion) with GPU work (inference). While the GPU processes batch $N$, the CPU selects leaf nodes for batch $N+1$. This requires solving the **duplicate path problem**: without protection, multiple concurrent selections would target the same promising leaf before the first result returns, wasting simulations.

The solution has three components: `RayAsyncEngine` (orchestration), `BatchInferenceManager` (batching), and `RayInferenceClient` (dispatch).

<div class="diagram-embed-wrap">
  <Diagram04 />
</div>

<p class="diagram-embed-link">
  <a href="/diagrams/diagram_04" target="_blank" rel="noopener noreferrer">
    Open in a new tab
  </a>
</p>

## RayAsyncEngine

The engine orchestrates the main search loop with three phases per iteration:

**Selection phase**: Traverse the tree via UCB to find leaf nodes. Apply virtual loss (`pending_visits += 1` on every node in the path) to discourage future selections from following the same path. Encode the leaf state tensor and enqueue it to the batch manager. Guards prevent redundant work: skip if a leaf is already in-flight (`inflight_to_root` dict), and pause if queue pressure exceeds `max_pending` (backpressure).

**Dispatch phase**: Send ready batches to Ray actors (delegates to `BatchInferenceManager`).

**Drain phase**: Collect results via `ray.wait(num_returns=1)` — returns as soon as **any** batch completes, not waiting for all. For each result: expand the leaf (create children from policy), backup the value to root (sign-flipping propagation), and remove virtual loss (`pending_visits -= 1`). Freed in-flight slots trigger `dispatch_ready()` to send more batches immediately.

Source: [`alphazero/gomoku/pvmcts/search/ray/ray_async.py:218-246`](https://github.com/sungyongcho/gomoku/blob/main/alphazero/gomoku/pvmcts/search/ray/ray_async.py#L218-L246)

```python
# alphazero/gomoku/pvmcts/search/ray/ray_async.py:218-246
# Virtual loss application + enqueue with error rollback
for node in path:
    node.pending_visits += 1
try:
    encoded = self.game.get_encoded_state(leaf.state)
    tensor = torch.from_numpy(encoded).unsqueeze(0)
    self.manager.enqueue(tensor, leaf, root, path)
except Exception:
    # Rollback virtual loss if enqueue fails
    for node in path:
        node.pending_visits = max(0, node.pending_visits - 1)
    raise
```

This shows the careful engineering: apply virtual loss along the entire path, enqueue the leaf for inference, and if anything fails, rollback to prevent stale `pending_visits > 0` from corrupting future selections.

## BatchInferenceManager

The manager accumulates individual leaf tensors into batches and dispatches them to GPU actors:

Source: [`alphazero/gomoku/pvmcts/search/ray/batch_inference_manager.py:154-161`](https://github.com/sungyongcho/gomoku/blob/main/alphazero/gomoku/pvmcts/search/ray/batch_inference_manager.py#L154-L161)

```python
# alphazero/gomoku/pvmcts/search/ray/batch_inference_manager.py:154-161
def _should_dispatch(self) -> bool:
    """Check whether dispatch conditions are satisfied."""
    if len(self._queue) >= self.batch_size:
        return True
    if self.max_wait_ns <= 0 or self._queue_start_ns is None:
        return False
    elapsed = time.monotonic_ns() - self._queue_start_ns
    return elapsed >= self.max_wait_ns
```

The dispatch condition uses OR logic: fire when the queue reaches `batch_size` **or** when the queue wait crosses the configured timeout. Internally, timeout is tracked as `max_wait_ns` against `_queue_start_ns` (`time.monotonic_ns()`), derived from user-facing `max_wait_ms`. This prevents latency spikes when only a few leaves are queued near the end of search.

The manager also enforces `max_inflight_batches` — when too many batches are in-flight, the selection phase pauses (backpressure: `max_pending = inflight_limit x batch_size`). This prevents unbounded memory growth from queued tensors.

## RayInferenceClient

The client dispatches inference requests to remote Ray GPU actors:

Source: [`alphazero/gomoku/inference/ray_client.py:289-315`](https://github.com/sungyongcho/gomoku/blob/main/alphazero/gomoku/inference/ray_client.py#L289-L315)

```python
# alphazero/gomoku/inference/ray_client.py:289-315
def infer_async(
    self,
    states: torch.Tensor,
    native_payload: list[object] | None = None,
    model_slot: str | None = None,
) -> ray.ObjectRef:
    """Run asynchronous inference and return an ObjectRef immediately."""
    if isinstance(states, np.ndarray):
        batch_np = states
    elif states.dim() in (3, 4):
        batch_np = states.detach().cpu().numpy()
    else:
        raise ValueError(f"Unsupported state shape: {states.shape}")

    # Least-loaded actor selection: pick the actor with fewest pending
    # requests to avoid stalling behind a slow actor.
    min_idx = 0
    min_pending = self._pending_counts[0]
    for i in range(1, len(self.actors)):
        if self._pending_counts[i] < min_pending:
            min_pending = self._pending_counts[i]
            min_idx = i
    self._pending_counts[min_idx] += 1
    actor = self.actors[min_idx]
    ref = actor.infer.remote(batch_np, native_payload, model_slot=model_slot)
    self._ref_to_actor[ref] = min_idx
    return ref
```

**Least-loaded actor selection** picks the actor with the fewest pending requests, distributing load evenly across GPU workers. The method accepts optional `native_payload` and `model_slot`, and returns a `ray.ObjectRef` immediately (non-blocking) so the caller proceeds with tree traversal while Ray handles compute.

On the actor side, an AsyncIO consumer loop implements aggressive batching: it blocks on the first item, then non-blocking grabs until `batch_size` is reached or a deadline expires. A single `model(inputs)` forward pass runs over the concatenated batch, then results are split back to per-request futures.

## Virtual Loss Mechanism

Virtual loss is the mechanism that makes asynchronous MCTS correct, not just fast.

When a leaf is selected and enqueued for inference, every node on the path from root to leaf gets `pending_visits += 1`. The UCB calculation includes these pending visits:

$$
UCB = Q + c \cdot P \cdot \frac{\sqrt{N + N_{\text{pending}}}}{1 + N_{\text{child}} + N_{\text{child\_pending}}}
$$

This inflates the "visited" count for nodes along the path, making that path less attractive for the next selection. Combined with the `inflight_to_root` dict (which prevents selecting the exact same leaf twice), this naturally spreads concurrent selections across diverse paths in the tree.

When inference results return, `pending_visits -= 1` on all path nodes. Error handling is critical: rollback uses `max(0, pending_visits - 1)` to guard against negative counts. A `finally` block in `search()` cleans up all outstanding virtual loss on early exit (KeyboardInterrupt, exception), preventing stale `pending_visits > 0` from corrupting future searches on the same tree.
