---
group: "Alphazero"
icon: "pi-sync"
---

# Training Pipeline and GCP Infrastructure

## Self-Play Loop

Each training iteration begins with the current champion model playing games against itself to generate training data. Each game produces a sequence of $(s, \pi, z)$ tuples: the board state $s$, the MCTS policy $\pi$ (normalized visit counts at the root), and the eventual game outcome $z$ ($+1$ win, $-1$ loss, $0$ draw).

Games are played with exploration enabled: Dirichlet noise at the root ensures the search doesn't collapse to a single line of play, and temperature $\tau = 1.0$ for the first 20 moves produces diverse opening positions. After the exploration phase, temperature drops to 0 (deterministic play) for the remainder of the game.

Opponent diversity prevents overfitting to a single play style. The self-play mix is configurable — for example, 5% random bot, 30% previous champion, 65% current champion — ensuring the training data covers a range of opponent strengths.

## Replay Buffer and Data Augmentation

Game records are stored as Parquet shards — a columnar format efficient for large datasets with mixed types (board as int8 buffer, policy as float16). The replay buffer holds up to 500K samples with FIFO eviction, and training only begins after a minimum sample count is reached.

**D4 symmetry augmentation** is the most impactful data engineering decision. The 19x19 board has 8 symmetries under the dihedral group $D_4$: 4 rotations ($0\degree$, $90\degree$, $180\degree$, $270\degree$) times 2 reflections (original and horizontally flipped). Each game sample is augmented 8x by applying a random symmetry transform to both the board state and the policy vector:

```python
# alphazero/gomoku/alphazero/learning/dataset.py:63-86
def _apply_symmetry(self, state: np.ndarray, policy: np.ndarray,
                    k: int) -> tuple[np.ndarray, np.ndarray]:
    if k >= 4:
        state = np.flip(state, axis=-1)
        policy = np.flip(policy.reshape(h, w), axis=-1)
    if k % 4 > 0:
        state = np.rot90(state, k=k % 4, axes=(-2, -1))
        policy = np.rot90(policy.reshape(h, w), k=k % 4)
    return np.ascontiguousarray(state), policy.flatten()
```

This effectively multiplies the training dataset by 8x without additional self-play compute — critical for data efficiency when GPU time is expensive.

<!-- DIAGRAM: D4 symmetry group. Show a small board (5x5 for clarity) and its 8 symmetry transformations: original, 90, 180, 270 rotations, plus horizontal flip of each. -->

## Prioritized Experience Replay (PER)

After iteration 15, samples are weighted by prediction error. Positions where the model was most wrong are sampled more frequently:

$$P(i) = \frac{p_i^\alpha}{\sum_k p_k^\alpha}, \quad w_i = (N \cdot P(i))^{-\beta}$$

where $p_i$ is the priority (TD-error + $\epsilon$), $\alpha$ controls how strongly priorities affect sampling, and $w_i$ is the importance-sampling correction weight that prevents biased gradient updates.

A critical design requirement: PER only works if sample priorities are **updated after each training batch** based on the prediction error from that batch. Without this update, the priority weighting becomes stale and provides no benefit — it's just random sampling with extra overhead. The implementation updates priorities in-place:

```python
# alphazero/gomoku/alphazero/learning/trainer.py:221-235
# Update PER priorities with fresh TD-error estimates
if use_per and getattr(dataset, "priorities", None) is not None:
    with torch.no_grad():
        td_errors = torch.abs(
            out_value.detach().squeeze() - value_targets.detach().squeeze()
        )
        new_priorities = (td_errors + per_eps).cpu().tolist()
    for idx_val, p_val in zip(batch_indices, new_priorities):
        dataset.priorities[int(idx_val)] = float(p_val)
```

## Training Loop

Each iteration runs 2 epochs over the replay buffer (the AlphaZero standard — prevents overfitting to the current buffer contents). Batch size 512, AMP (automatic mixed precision) for GPU efficiency, Adam optimizer with scheduled learning rate.

The loss combines value MSE and policy cross-entropy:

```python
# Policy: cross-entropy with MCTS visit targets
policy_losses = -(policy_targets * log_softmax(out_policy)).sum(dim=1)
# Value: MSE between prediction and game outcome
value_losses = mse_loss(out_value.squeeze(), value_targets.squeeze())
# Combined
loss = (policy_losses + value_losses).mean()
```

## Scheduled Parameters

Many hyperparameters change over the course of training. The config system (Pydantic models) supports scheduled values — parameters that vary by training iteration:

```yaml
# alphazero/configs/ — example scheduled parameter
learning_rate:
  - { until: 30, value: 0.002 }
  - { until: 137, value: 0.0008 }
```

This enables learning rate warmup/decay, increasing MCTS simulations as the model improves, and temperature annealing — all configured declaratively in YAML without code changes.

## GCP Training Infrastructure

Training ran on a GCP Ray cluster with GPU workers. Ray handles distributed self-play: multiple CPU workers generate games in parallel, sending inference requests to GPU actors via the async pipeline described in [Search Engine Architecture](/docs/alphazero/search-engine-architecture).

**Training progression observations**: The model started exhibiting blocking behavior (defending against opponent threats) around training day 3 of the `elo1800-v4` run. As iterations progressed, it developed aggressive play patterns — including creating open threes (3-3 patterns) that force the opponent into defensive positions. The current champion model plays recognizably strategic Gomoku.

Data is stored in GCS and checkpoints are saved per iteration. The manifest (`manifest.json`) tracks training metadata, promotion history, and config snapshots for reproducibility — making it possible to resume training or audit any iteration's state.
