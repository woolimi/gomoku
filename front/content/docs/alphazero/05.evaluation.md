---
group: "Alphazero"
icon: "pi-chart-line"
---

# Model Evaluation: Arena and SPRT

## The Promotion Gate

After training produces a new model (the "challenger"), it must prove it's stronger than the current best (the "champion") in head-to-head matches before replacing it. This gate exists to prevent a failure mode: if a weaker challenger replaced the champion, all subsequent self-play games would use that weaker model — generating lower-quality training data. This compounds: worse data produces a worse next model, which produces even worse data. The arena evaluation prevents this regression loop.

## Arena Evaluation

The arena runs 40 head-to-head games between challenger and champion, split evenly: 20 games with the challenger playing Black (Player 1), 20 with the challenger playing White (Player 2). This alternation ensures the evaluation isn't biased by first-move advantage.

Evaluation uses deterministic play for maximum signal: temperature $\tau = 0$ (argmax — always pick the most-visited move), no Dirichlet noise, and 600 MCTS simulations per move (3x the training budget for more accurate play).

The promotion threshold is a 55% win rate. The evaluation also tracks guardrails beyond raw win rate:

- **Baseline win rate**: if configured, the challenger must also beat a baseline model (e.g., an early checkpoint) above a minimum threshold — preventing catastrophic forgetting
- **Blunder rate**: tracks how often the challenger makes moves that drastically drop its own position evaluation — a proxy for tactical instability

All three conditions must pass for promotion:

```python
# alphazero/gomoku/alphazero/eval/arena.py:363-367
summary["promote"] = bool(
    summary["pass_promotion"]      # win_rate >= 55%
    and summary["pass_baseline"]   # baseline_wr >= minimum
    and summary["pass_blunder"]    # blunder_rate <= limit
)
```

## SPRT: Early Stopping

SPRT (Sequential Probability Ratio Test) was implemented to terminate arena matches early once statistical significance is reached — no need to play all 40 games if the result is already clear.

The log-likelihood ratio after $n$ games with win rate $w$:

$$LLR = n \cdot \left[ w \cdot \ln\frac{p_1}{p_0} + (1-w) \cdot \ln\frac{1-p_1}{1-p_0} \right]$$

where $p_0$ is the null hypothesis win rate (challenger is not better) and $p_1$ is the alternative hypothesis (challenger is better).

```python
# alphazero/gomoku/alphazero/eval/sprt.py:7-36
def check_sprt(cfg, wins, losses, draws):
    n = wins + losses
    wr = wins / n if n > 0 else 0.0

    llr = n * (
        wr * math.log(cfg.p1 / cfg.p0 + 1e-12)
        + (1 - wr) * math.log((1 - cfg.p1) / (1 - cfg.p0) + 1e-12)
    )

    lower = math.log(cfg.beta / (1 - cfg.alpha))
    upper = math.log((1 - cfg.beta) / cfg.alpha)

    if llr > upper:
        return "accept_h1"  # promote challenger
    if llr < lower:
        return "accept_h0"  # keep champion
    return "continue"       # keep playing
```

If $LLR$ exceeds the upper boundary: accept the challenger (promote). If it drops below the lower boundary: reject (keep champion). Otherwise: keep playing.

SPRT was implemented and validated in the original repository but was not used during the `elo1800-gcp-v4` training runs — arena evaluation used fixed game counts (40 games per evaluation) instead. The fixed-count approach was simpler to reason about and provided consistent evaluation signal across all iterations.

## Elo Tracking

Cumulative Elo is tracked across training iterations as an internal progress metric:

$$\Delta\text{Elo} = 400 \cdot \log_{10}\left(\frac{w}{1-w}\right)$$

where $w$ is the challenger's win rate against the champion. Each successful promotion adds a positive delta.

**Important caveat**: this Elo is self-referential — the new champion inherits the old champion's score, and the number can only increase as long as promotions succeed. It is not calibrated against any external benchmark and should not be interpreted as an absolute skill rating. The real evidence of model strength is behavioral: the champion model blocks opponent threats, creates capture opportunities, and plays aggressive 3-3 patterns — observable by playing against it at [sungyongcho.com/gomoku](https://sungyongcho.com/gomoku).
