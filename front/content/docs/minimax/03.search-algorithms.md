---
group: "minimax"
icon: "pi-search"
---

# Search: From Alpha-Beta to PVS

## Alpha-Beta Pruning

$$
v_{\alpha\beta}(s)=
\begin{cases}
\max_{a\in A(s)} v_{\alpha\beta}(s_a) & \text{(MAX)} \\
\min_{a\in A(s)} v_{\alpha\beta}(s_a) & \text{(MIN)}
\end{cases}
\quad \text{with cutoff if } \alpha \ge \beta
$$

Alpha-beta pruning is the foundational optimization over naive minimax. The idea: maintain a window $[\alpha, \beta]$ representing the best scores each player can already guarantee. If a node's score can't possibly influence the final decision — because it falls outside this window — prune the remaining children.

The cutoff condition is straightforward: if $\alpha \geq \beta$, stop searching siblings. With perfect move ordering, this reduces the effective branching factor from $O(b^d)$ to $O(b^{d/2})$ — searching to depth 10 with the same cost as naive minimax at depth 5.

This was the first search algorithm implemented. It improved over naive minimax dramatically, but wasn't enough for depth 10 on a 19x19 board in reasonable time. The key bottleneck wasn't the algorithm — it was move ordering. Alpha-beta only prunes well when the best move is examined first.

<div class="diagram-embed-wrap">
  <Diagram09 />
</div>

<p class="diagram-embed-link">
  <a href="/diagrams/diagram_09" target="_blank" rel="noopener noreferrer">
    Open in a new tab
  </a>
</p>


## Move Ordering

Alpha-beta's effectiveness depends entirely on examining the best move first. The engine uses a three-tier ordering system:

1. **TT hash move**: If a previous search found a best move for this position (stored in the transposition table), try it first. This is often the principal variation move from a shallower iteration, causing immediate cutoffs.

2. **Killer moves**: Two moves per depth level that caused beta cutoffs at sibling nodes. The heuristic: a move that refutes one sibling often refutes others. Stored in `killerMoves[MAX_DEPTH+1][2]`, updated on every cutoff:

Source: [`minimax/src/gomoku/search/Minimax.cpp:357-360`](https://github.com/sungyongcho/gomoku/blob/main/minimax/src/gomoku/search/Minimax.cpp#L357-L360)

```cpp
// minimax/src/gomoku/search/Minimax.cpp:357-360
if (killerMoves[depth][0] != mv && killerMoves[depth][1] != mv) {
  killerMoves[depth][1] = killerMoves[depth][0];
  killerMoves[depth][0] = mv;
}
```

Just four lines — shift the old killer out, insert the new one — but this simple heuristic provides substantial pruning improvements by ensuring refutation moves are tried early.

3. **Evaluation-sorted**: Remaining moves are scored by the evaluation function and sorted descending. Expensive per-node, but the pruning it enables more than compensates.

This ordering is what makes depth 10 feasible. Without it, even alpha-beta can't overcome the 200+ branching factor of a 19x19 board.

## Transposition Table

Positions can be reached via different move orders but produce identical game states. The transposition table (TT) stores previously evaluated positions to avoid redundant work:

Source: [`minimax/src/gomoku/search/Minimax.cpp:260-285`](https://github.com/sungyongcho/gomoku/blob/main/minimax/src/gomoku/search/Minimax.cpp#L260-L285)

```cpp
// minimax/src/gomoku/search/Minimax.cpp:260-285
inline bool probeTT(Board *board, int depth, int &alpha, int &beta,
                    std::pair<int, int> &bestMove, int &scoreOut) {
  uint64_t h = board->getHash();
  boost::unordered_map<uint64_t, TTEntry>::iterator it = transTable.find(h);
  if (it == transTable.end()) return false;

  const TTEntry &e = it->second;
  bestMove = e.bestMove;

  if (e.depth < depth) return false;  // only ordering info
  scoreOut = e.score;

  if (e.flag == EXACT) {
    board->flushCaptures();
    return true;
  }
  if (e.flag == LOWERBOUND)
    alpha = std::max(alpha, scoreOut);
  else /*UPPERBOUND*/
    beta = std::min(beta, scoreOut);
  if (alpha >= beta) {
    board->flushCaptures();
    return true;
  }
  return false;
}
```

Each entry stores `{score, depth, bestMove, boundType}`, keyed by Zobrist hash. Three bound types are used:
- **EXACT**: score was within the alpha-beta window — the true minimax value
- **LOWERBOUND**: score $\geq \beta$, caused a cutoff — we know the score is at least this high
- **UPPERBOUND**: score $\leq \alpha$, all moves failed low — we know the score is at most this high

On lookup, the stored bound narrows the current window. If the narrowed window causes $\alpha \geq \beta$, the position is resolved without searching. Even when the stored depth is too shallow for a full cutoff, the best move is still used for move ordering — providing the hash move for tier 1.

## Iterative Deepening

Rather than jumping straight to the target depth, the engine searches depth 1, then depth 2, then depth 3... up to `maxDepth` or a time limit. This might seem wasteful — repeating shallower searches — but each iteration populates the transposition table, so deeper iterations benefit from much better move ordering. The TT's best moves from depth $d-1$ become the hash moves at depth $d$, dramatically improving pruning.

Medium difficulty uses iterative deepening with a 0.4-second time limit. On timeout, the engine returns the best move from the deepest completed iteration — ensuring it always has a valid response regardless of how deep it managed to search.

## Quiescence Search

At depth 0, instead of returning a static evaluation, the engine continues searching **capture-only moves**. This prevents the "horizon effect" — where a devastating capture lurks just beyond the search depth, invisible to the static evaluation.

The stand-pat score (static evaluation at depth 0) serves as a lower bound: the current player can always choose not to capture. Only if a capture improves the score does the search continue deeper. When no captures remain, the position is "quiet" and the static evaluation is reliable.

This matters particularly for Gomoku with capture rules: a position that looks equal statically might have a forced capture sequence that swings the evaluation dramatically. Without quiescence search, the engine would frequently walk into capture traps at the search horizon.

## Principal Variation Search (PVS)

PVS builds on a key insight: with good move ordering, the first child at each node is usually the best move (the "principal variation"). Instead of searching every child with the full $[\alpha, \beta]$ window, PVS searches the first child normally, then uses a **zero-width null window** $[\alpha+1, \alpha+1]$ for all subsequent children — essentially asking "is this move better than my current best?"

Equivalent formulations in other references often write this as $[\alpha, \alpha+1]$; the implementation here uses the $[\alpha+1, \alpha+1]$ form.

If a null-window search returns a score $> \alpha$ and $< \beta$, the PV assumption was wrong: this child is better than expected. The engine re-searches with the full window to get the exact score. In practice, re-searches are rare when move ordering is good, making PVS faster than standard alpha-beta.

<div class="diagram-embed-wrap">
  <Diagram10 />
</div>

<p class="diagram-embed-link">
  <a href="/diagrams/diagram_10" target="_blank" rel="noopener noreferrer">
    Open in a new tab
  </a>
</p>


Source: [`minimax/src/gomoku/search/Minimax.cpp:580-652`](https://github.com/sungyongcho/gomoku/blob/main/minimax/src/gomoku/search/Minimax.cpp#L580-L652)

```cpp
// minimax/src/gomoku/search/Minimax.cpp:580-652
int pvs(Board *board, int depth, int alpha, int beta, int currentPlayer,
        int lastX, int lastY, bool isMaximizing, EvalFn evalFn) {
  // ... TT probe, terminal check ...

  bool firstChild = true;
  for (size_t i = 0; i < scored.size(); ++i) {
    const std::pair<int, int> &mv = scored[i].move;
    UndoInfo ui = board->makeMove(mv.first, mv.second);
    int next = board->getNextPlayer();

    int score;
    if (firstChild) {
      // full window
      score = pvs(board, depth - 1, alpha, beta, next,
                  mv.first, mv.second, !isMaximizing, evalFn);
      firstChild = false;
    } else {
      // null window
      score = pvs(board, depth - 1, alpha + 1, alpha + 1, next,
                  mv.first, mv.second, !isMaximizing, evalFn);
      // if it produced something interesting, re-search
      if (score > alpha && score < beta) {
        score = pvs(board, depth - 1, alpha, beta, next,
                    mv.first, mv.second, !isMaximizing, evalFn);
      }
    }

    board->undoMove(ui);
    updateBestAndBounds(isMaximizing, score, mv, bestEval, bestMove, alpha, beta);
    if (alpha >= beta) {
      // update killer moves on cutoff
      if (!isKillerMove(depth, mv)) {
        killerMoves[depth][1] = killerMoves[depth][0];
        killerMoves[depth][0] = mv;
      }
      break;
    }
  }

  storeTT(hash, depth, bestMove, bestEval, alphaOrig, beta);
  return bestEval;
}
```

PVS was implemented with help from the [chessprogramming.org PVS article](https://www.chessprogramming.org/Principal_Variation_Search) for the null-window re-search logic. Understanding the theory was one thing; getting the sign conventions and bound types right in practice was the real challenge.

## Result

Benchmarks were measured on an École 42 Paris lab Dell OptiPlex 7400 AIO workstation (12th Gen Intel Core i7-12700, 12 cores / 20 threads, 15 GiB RAM), running Ubuntu 22.04.4 LTS (kernel 5.15.0-170-generic), with `g++ 10.5.0` and CPU governor `powersave`. Measurements used the release build (`-O2`) and single-thread execution (`./search_benchmark`).

Across repeated runs on the opening scenario (~200 legal moves), plain alpha-beta at depth 5 (`easy`) took about 4.07s, while PVS at depth 10 (`hard`) took about 2.33s. `medium` (iterative deepening with a 0.4s time limit) stayed near 0.40s. As the board gets denser, `easy` scales poorly (13.6 to 29.2s), while `hard` remains much lower (0.92 to 4.71s), confirming that move ordering + TT + PVS provide large practical gains. These benchmarks are directly reproducible by running `make benchmark && ./search_benchmark` in [`minimax/`](https://github.com/sungyongcho/gomoku/tree/main/minimax).

Observed ranges from repeated runs (`iterations=1`, `warmup=0`):

| Scenario | easy (ms) | medium (ms) | hard (ms) |
| --- | ---: | ---: | ---: |
| opening | 4066.57 - 4077.10 | 401.13 - 405.25 | 2330.71 - 2331.39 |
| midgame | 13607.93 - 13796.55 | 404.99 - 409.15 | 920.02 - 932.39 |
| late_midgame | 28588.11 - 29238.21 | 437.79 - 440.14 | 4706.83 - 4711.01 |
