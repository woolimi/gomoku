---
group: "AlphaZero"
icon: "pi-sitemap"
---

# Monte Carlo Tree Search with PUCT

## MCTS vs Minimax

Unlike minimax, which exhaustively searches to a fixed depth with a uniform branching strategy, MCTS builds a search tree **asymmetrically** — spending more time on promising moves and less on obviously bad ones. There's no fixed search depth; instead, the algorithm runs a fixed number of simulations, each traversing from root to a leaf, evaluating, and backing up.

After $N$ simulations, the action policy at the root is proportional to visit counts: $\pi(a) \propto N(a)$. More visits to a child node means more confidence that it's a good move.

## Four Phases

Each MCTS simulation has four phases:

1. **Select**: Starting from the root, traverse down the tree by choosing the child with the highest UCB score at each node. This balances exploitation (high value) with exploration (low visit count, high prior).

2. **Expand**: When a leaf node is reached (a node with no children), expand it by creating child nodes for all legal moves. Each child's prior probability comes from the neural network's policy output.

3. **Evaluate**: Run neural network inference on the leaf state to get a policy vector (priors for each legal move) and a value estimate (who's winning).

4. **Backup**: Propagate the value estimate from the leaf back to the root. At each level, the sign flips — the opponent's gain is our loss:

```python
# alphazero/gomoku/pvmcts/treenode.py:217-235
def backup(self, value: float) -> None:
    node = self
    while node is not None:
        node.value_sum += value
        node.visit_count += 1
        value = -value  # flip for opponent
        node = node.parent
```

<!-- DIAGRAM: MCTS 4-phase loop. Show a tree with root and children. Label the four phases with arrows: (1) Select path down to leaf, (2) Expand leaf into children, (3) Evaluate leaf with NN → policy + value, (4) Backup value to root with alternating signs. -->

## PUCT Selection Formula

The selection step uses the PUCT (Predictor + UCB applied to Trees) formula:

$$
UCB(s,a) = Q(s,a) + c_{\text{puct}} \cdot P(s,a) \cdot \frac{\sqrt{N(s)}}{1 + N(s,a)}
$$

where:
- $Q(s,a)$ = mean value of the child (average backed-up value)
- $P(s,a)$ = prior probability from the neural network's policy head
- $N(s)$ = visit count of the parent
- $N(s,a)$ = visit count of the child
- $c_{\text{puct}}$ = exploration constant (2.0 in this implementation)

This formula balances **exploitation** (high $Q$ — moves that have led to good outcomes) with **exploration** (high prior $P$, low visits $N(s,a)$ — moves the network thinks are promising but haven't been explored much yet). The neural network's policy prior guides the search toward promising moves before they've been visited enough to have reliable $Q$ values — this is what makes MCTS + neural network so much more powerful than MCTS with random rollouts.

```python
# alphazero/gomoku/pvmcts/treenode.py:102-128
def get_ucb(self, c_puct: float) -> float:
    if self.parent is None:
        return 0.0
    parent_visits = self.parent.visit_count + self.parent.pending_visits
    q_value = 0.0
    total_child_visits = self.visit_count + self.pending_visits
    if total_child_visits > 0:
        q_value = self.value_sum / total_child_visits
    q_value = -q_value  # opponent's perspective
    exploration = (c_puct * self.prior
                   * math.sqrt(parent_visits)
                   / (1 + total_child_visits))
    return q_value + exploration
```

Note the sign flip: `q_value = -q_value` because the child represents the opponent's perspective. A high value for the child is bad for the parent.

## Dirichlet Noise

At the root node only, Dirichlet noise is added to the priors before search begins:

$$
P'(s,a) = (1 - \epsilon) \cdot P(s,a) + \epsilon \cdot \eta_a, \quad \eta \sim \text{Dir}(\alpha)
$$

This ensures exploration during self-play training: even if the network strongly favors one move, the noise gives other moves a chance to be explored. Without noise, the search would converge to the network's current beliefs and never discover better moves.

Parameters: $\epsilon = 0.25$, $\alpha = 0.15$. These values are from the AlphaZero paper, tuned for the Go/Chess action space scale.

## Temperature-Based Action Selection

After MCTS completes, the root visit counts form a policy distribution. How this distribution is used depends on the training phase:

$$
\pi(a) \propto N(a)^{1/\tau}
$$

During the first 20 moves (configurable `exploration_turns`), temperature $\tau = 1.0$ — actions are sampled proportionally to visit counts, encouraging diverse opening play for the training data. After the exploration phase, $\tau \to 0$ (effectively argmax — pick the most-visited move), which produces the strongest deterministic play.

During evaluation and production serving, temperature is always 0 and Dirichlet noise is disabled — the system plays at full strength.
