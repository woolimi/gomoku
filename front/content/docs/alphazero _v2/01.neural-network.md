---
group: "Alphazero"
icon: "pi-microchip"
---

# The Policy-Value Network

## Dual-Head Architecture

A single convolutional neural network processes the board state and produces two outputs simultaneously:

- **Policy head**: a probability distribution over 361 actions (where to play next)
- **Value head**: a scalar in $[-1, 1]$ predicting who is winning from the current position

This replaces both components of classical minimax: the evaluation function (value head) and move ordering (policy head). The combined loss function trains both heads jointly:

$$L = (z - v)^2 - \pi^T \log p + c\|\theta\|^2$$

where $z$ is the game outcome ($+1$ win, $-1$ loss), $v$ is the predicted value, $\pi$ is the MCTS policy (normalized visit counts), $p$ is the network's policy output, and $c\|\theta\|^2$ is L2 regularization. The first term pushes the value head to predict game outcomes; the second term pushes the policy head to match the MCTS-refined move distribution.

## ResNet Architecture

The network uses a residual architecture with the following structure:

- **Start block**: `Conv2d(13→128, 3x3, pad=1)` → `BatchNorm2d` → `ReLU`
- **12 residual blocks**, each:
  - `Conv2d(128→128, 3x3, pad=1)` → `BatchNorm2d` → `ReLU`
  - `Conv2d(128→128, 3x3, pad=1)` → `BatchNorm2d`
  - Skip connection (add input) → `ReLU`
- **Policy head**: `Conv2d(128→2, 1x1)` → `BatchNorm2d` → `ReLU` → Flatten$(2 \times 19 \times 19 = 722)$ → `Linear(722→361)` → logits
- **Value head**: `Conv2d(128→1, 1x1)` → `BatchNorm2d` → `ReLU` → Flatten$(361)$ → `Linear(361→256)` → `ReLU` → `Linear(256→1)` → `Tanh`

```python
# alphazero/gomoku/model/policy_value_net.py:68-139
class PolicyValueNet(nn.Module):
    def __init__(self, game, model_config, device):
        super().__init__()
        self.device = device
        num_planes = model_config.num_planes  # 13
        num_hidden = model_config.num_hidden   # 128
        num_resblocks = model_config.num_resblocks  # 12

        self.start_block = nn.Sequential(
            nn.Conv2d(num_planes, num_hidden, kernel_size=3, padding=1),
            nn.BatchNorm2d(num_hidden),
            nn.ReLU(),
        )
        self.backbone = nn.ModuleList(
            [ResBlock(num_hidden) for _ in range(num_resblocks)]
        )
        # Policy head → 361 logits
        # Value head → scalar in [-1, 1]
        ...
```

The original AlphaZero paper used 256 channels and 19–39 residual blocks with TPU-scale compute. 128 channels and 12 blocks is the balance point for the available GPU budget while still providing sufficient model capacity for 19x19 Gomoku.

<!-- DIAGRAM: Network architecture. Show vertically: Input (13x19x19) → Start Block → 12x Residual Block → split into Policy Head (→ 361 logits) and Value Head (→ scalar). Label layer sizes at each stage. -->

## State Encoding

The network sees a 13-channel tensor, each channel being a 19x19 binary or scalar plane:

| Channels | Description |
|----------|-------------|
| 0 | Current player's stones (binary) |
| 1 | Opponent's stones (binary) |
| 2 | Empty positions (binary) |
| 3 | Last move indicator (one-hot) |
| 4 | Current player's capture score (ratio, clipped 0–1) |
| 5 | Opponent's capture score (ratio, clipped 0–1) |
| 6 | Color indicator (1.0 for Black, -1.0 for White) |
| 7 | Forbidden move mask (double-three positions, binary) |
| 8–12 | Move history (last 5 moves as one-hot planes) |

```python
# alphazero/gomoku/core/gomoku.py:361-464 (get_encoded_state)
# Channel 0: current player stones
# Channel 1: opponent stones
# Channel 2: empty
# Channel 3: last move one-hot
# Channel 4-5: capture scores as ratios
# Channel 6: color plane (constant 1.0 or -1.0)
# Channel 7: forbidden points
# Channels 8-12: move history
```

This encoding gives the network everything it needs: the current position, game-specific context (captures, forbidden moves), temporal context (move history), and player identity. The capture score channels are particularly important for this Gomoku variant — they tell the network how close each player is to the 5-pair capture win condition.

The `get_encoded_state` method supports batch encoding and D4 symmetry augmentation, and has an optional native C++ encoding path for faster inference in production.
